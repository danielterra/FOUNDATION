#!/usr/bin/env node

/**
 * FOUNDATION Database Builder
 *
 * Builds the complete FOUNDATION.db with:
 * 1. Schema (tables, indices, views)
 * 2. RDF/RDFS/OWL core ontology
 * 3. WordNet synsets as OWL classes
 *
 * This script is run during development setup and before releases.
 * The resulting database is versioned in git.
 */

const Database = require('better-sqlite3');
const fs = require('fs');
const path = require('path');
const { createReadStream } = require('fs');
const { createInterface } = require('readline');

const PROJECT_ROOT = path.join(__dirname, '..');
const DB_PATH = path.join(PROJECT_ROOT, 'FOUNDATION.db');
const SCHEMA_PATH = path.join(PROJECT_ROOT, 'db', 'schema.sql');
const RDF_CORE_PATH = path.join(PROJECT_ROOT, 'core-ontology', 'rdf-rdfs-owl-core.ttl');
const WORDNET_PATH = path.join(PROJECT_ROOT, 'core-ontology', 'english-wordnet-2024.ttl');

const SYNSET_LIMIT = 100; // Import first 100 synsets for testing

async function main() {
  console.log('ðŸš€ FOUNDATION Database Builder\n');
  console.log('================================\n');

  // Step 1: Delete old database
  if (fs.existsSync(DB_PATH)) {
    console.log('ðŸ—‘ï¸  Deleting old database...');
    fs.unlinkSync(DB_PATH);
  }

  // Step 2: Create database and schema
  console.log('ðŸ“‹ Creating schema...');
  const db = new Database(DB_PATH);
  const schema = fs.readFileSync(SCHEMA_PATH, 'utf-8');
  db.exec(schema);
  console.log('âœ… Schema created\n');

  // Step 3: Import RDF/RDFS/OWL core ontology
  console.log('ðŸ“š Importing RDF/RDFS/OWL core ontology...');
  const coreTriples = await importTurtleFile(RDF_CORE_PATH, db, 1, 'core');
  console.log(`âœ… Imported ${coreTriples} triples from RDF/RDFS/OWL\n`);

  // Step 4: Import WordNet synsets
  console.log(`ðŸ“– Importing first ${SYNSET_LIMIT} WordNet synsets...`);
  const wordnetTriples = await importWordNetSynsets(WORDNET_PATH, db, coreTriples + 1, 'core');
  console.log(`âœ… Imported ${wordnetTriples} triples from WordNet\n`);

  // Step 5: Set metadata
  console.log('âš™ï¸  Setting metadata...');
  db.prepare("UPDATE metadata SET value = 'true', updated_at = ? WHERE key = 'ontology_imported'")
    .run(Date.now());
  console.log('âœ… Metadata updated\n');

  // Step 6: Print statistics
  const stats = db.prepare(`
    SELECT
      (SELECT COUNT(*) FROM triples) as total_triples,
      (SELECT COUNT(*) FROM triples WHERE retracted = 0) as active_triples,
      (SELECT MAX(tx) FROM triples) as max_tx,
      (SELECT COUNT(DISTINCT subject) FROM triples WHERE retracted = 0) as entities
  `).get();

  console.log('================================');
  console.log('ðŸ“Š Database Statistics:\n');
  console.log(`   Total triples: ${stats.total_triples}`);
  console.log(`   Active triples: ${stats.active_triples}`);
  console.log(`   Transactions: ${stats.max_tx}`);
  console.log(`   Entities: ${stats.entities}`);

  const sizeBytes = fs.statSync(DB_PATH).size;
  const sizeKB = (sizeBytes / 1024).toFixed(2);
  console.log(`   Size: ${sizeKB} KB\n`);

  console.log(`âœ… Database ready: ${DB_PATH}`);

  db.close();
}

main().catch(err => {
  console.error('âŒ Database build failed:', err);
  process.exit(1);
});

// ============================================================================
// Helper Functions
// ============================================================================

/**
 * Import Turtle file using N3 parser
 */
async function importTurtleFile(filePath, db, startTx, origin) {
  const { default: N3Parser } = await import('@rdfjs/parser-n3');
  const { default: streamifyString } = await import('streamify-string');
  const rdfParser = new N3Parser();
  const content = fs.readFileSync(filePath, 'utf-8');
  const stream = rdfParser.import(streamifyString(content));

  let tx = startTx;
  let count = 0;
  const timestamp = Date.now();

  // Begin transaction
  const insertTriple = db.prepare(`
    INSERT INTO triples (
      subject, predicate, object, object_value, object_type, object_datatype,
      object_language, object_number, object_integer, object_datetime, object_boolean,
      tx, origin, retracted, created_at
    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, 0, ?)
  `);

  const insert = db.transaction((triples) => {
    for (const triple of triples) {
      insertTriple.run(
        triple.subject,
        triple.predicate,
        triple.object,
        triple.object_value,
        triple.object_type,
        triple.object_datatype,
        triple.object_language,
        triple.object_number,
        triple.object_integer,
        triple.object_datetime,
        triple.object_boolean,
        triple.tx,
        triple.origin,
        triple.created_at
      );
    }
  });

  // Parse and collect triples
  const triples = [];

  // Consume the stream
  for await (const quad of stream) {
    const subject = quad.subject.value;
    const predicate = quad.predicate.value;

    let object = null;
    let object_value = null;
    let object_type = null;
    let object_datatype = null;
    let object_language = null;
    let object_number = null;
    let object_integer = null;
    let object_datetime = null;
    let object_boolean = null;

    if (quad.object.termType === 'Literal') {
      object_type = 'literal';
      object_value = quad.object.value;
      object_datatype = quad.object.datatype?.value || 'http://www.w3.org/2001/XMLSchema#string';
      object_language = quad.object.language || null;

      // Parse typed values
      if (object_datatype.includes('decimal') || object_datatype.includes('double') || object_datatype.includes('float')) {
        object_number = parseFloat(object_value);
      } else if (object_datatype.includes('integer') || object_datatype.includes('int') || object_datatype.includes('long')) {
        object_integer = parseInt(object_value, 10);
      } else if (object_datatype.includes('boolean')) {
        object_boolean = object_value === 'true' ? 1 : 0;
      }
    } else if (quad.object.termType === 'BlankNode') {
      object_type = 'blank';
      object = quad.object.value;
    } else {
      object_type = 'iri';
      object = quad.object.value;
    }

    triples.push({
      subject,
      predicate,
      object,
      object_value,
      object_type,
      object_datatype,
      object_language,
      object_number,
      object_integer,
      object_datetime,
      object_boolean,
      tx: tx++,
      origin,
      created_at: timestamp
    });
    count++;
  }

  insert(triples);
  return count;
}

/**
 * Import WordNet synsets as OWL classes
 */
async function importWordNetSynsets(filePath, db, startTx, origin) {
  const synsets = await parseWordNetSynsets(filePath, SYNSET_LIMIT);

  let tx = startTx;
  let count = 0;
  const timestamp = Date.now();

  const insertTriple = db.prepare(`
    INSERT INTO triples (
      subject, predicate, object, object_value, object_type, object_datatype,
      object_language, object_number, object_integer, object_datetime, object_boolean,
      tx, origin, retracted, created_at
    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, 0, ?)
  `);

  const insert = db.transaction((triples) => {
    for (const triple of triples) {
      insertTriple.run(
        triple.subject,
        triple.predicate,
        triple.object,
        triple.object_value,
        triple.object_type,
        triple.object_datatype,
        null, // language
        null, // number
        null, // integer
        null, // datetime
        null, // boolean
        triple.tx,
        triple.origin,
        triple.created_at
      );
    }
  });

  const triples = [];

  for (const synset of synsets) {
    const className = `Concept_${synset.id.replace(/[^a-zA-Z0-9]/g, '_')}`;
    const classIRI = `http://FOUNDATION.local/ontology/${className}`;

    // Triple 1: Class declaration (rdf:type owl:Class)
    triples.push({
      subject: classIRI,
      predicate: 'http://www.w3.org/1999/02/22-rdf-syntax-ns#type',
      object: 'http://www.w3.org/2002/07/owl#Class',
      object_value: null,
      object_type: 'iri',
      object_datatype: null,
      tx: tx++,
      origin,
      created_at: timestamp
    });
    count++;

    // Triple 2: Label (rdfs:label) - extract noun from definition
    let label = synset.lemma;
    if (!label && synset.definition) {
      // Extract the main noun from definition
      const def = synset.definition;

      // Pattern: "a/an [adjectives...] NOUN" - capture the noun
      // Examples: "a living organism" -> "organism", "an entity that..." -> "entity"
      const match = def.match(/^(?:a|an)\s+(?:\([^)]+\)\s+)?(?:[\w-]+\s+)*([\w-]+?)(?:\s+that|\s+which|\s+of|\s+with|\s+in|\s*;|\s*,|\s*\(|$)/);
      if (match && match[1] && match[1].length > 2) {
        label = match[1];
      } else {
        // Fallback: use first 3 words
        const words = def.split(/\s+/);
        label = words.slice(0, Math.min(3, words.length)).join(' ');
      }
    }

    if (label) {
      triples.push({
        subject: classIRI,
        predicate: 'http://www.w3.org/2000/01/rdf-schema#label',
        object: null,
        object_value: label,
        object_type: 'literal',
        object_datatype: 'http://www.w3.org/2001/XMLSchema#string',
        tx: tx++,
        origin,
        created_at: timestamp
      });
      count++;
    }

    // Triple 3: Definition (rdfs:comment)
    if (synset.definition) {
      triples.push({
        subject: classIRI,
        predicate: 'http://www.w3.org/2000/01/rdf-schema#comment',
        object: null,
        object_value: synset.definition,
        object_type: 'literal',
        object_datatype: 'http://www.w3.org/2001/XMLSchema#string',
        tx: tx++,
        origin,
        created_at: timestamp
      });
      count++;
    }

    // Triple 4: Hypernyms (rdfs:subClassOf)
    // If no hypernyms, make it a direct subclass of owl:Thing
    if (synset.hypernyms.length === 0) {
      triples.push({
        subject: classIRI,
        predicate: 'http://www.w3.org/2000/01/rdf-schema#subClassOf',
        object: 'http://www.w3.org/2002/07/owl#Thing',
        object_value: null,
        object_type: 'iri',
        object_datatype: null,
        tx: tx++,
        origin,
        created_at: timestamp
      });
      count++;
    } else {
      // Add all hypernyms
      for (const hypernym of synset.hypernyms) {
        const parentClass = `http://FOUNDATION.local/ontology/Concept_${hypernym.replace(/[^a-zA-Z0-9]/g, '_')}`;
        triples.push({
          subject: classIRI,
          predicate: 'http://www.w3.org/2000/01/rdf-schema#subClassOf',
          object: parentClass,
          object_value: null,
          object_type: 'iri',
          object_datatype: null,
          tx: tx++,
          origin,
          created_at: timestamp
        });
        count++;
      }
    }
  }

  insert(triples);
  return count;
}

/**
 * Parse WordNet noun synsets from Turtle file
 * Only imports nouns (-n) as owl:Class entities
 */
async function parseWordNetSynsets(filePath, limit) {
  console.log('   Parsing WordNet file (nouns only)...');

  // First pass: build map of synset ID -> lemma
  const synsetToLemma = new Map();
  let fileStream = createReadStream(filePath);
  let rl = createInterface({ input: fileStream, crlfDelay: Infinity });

  let currentLemma = null;
  let currentWrittenRep = null;

  for await (const line of rl) {
    const trimmed = line.trim();

    // Lexical entry with lemma name in URI (nouns only)
    if (trimmed.startsWith('<https://en-word.net/lemma/')) {
      const match = trimmed.match(/lemma\/([^#]+)#\1-n>/);
      if (match) {
        currentLemma = match[1];
        currentWrittenRep = null;
      }
    }

    // Written representation (the actual word)
    if (currentLemma && trimmed.includes('ontolex:writtenRep')) {
      const match = trimmed.match(/"([^"]+)"/);
      if (match) {
        currentWrittenRep = match[1];
      }
    }

    // Sense linking to synset
    if (currentLemma && currentWrittenRep && trimmed.includes('ontolex:sense')) {
      const match = trimmed.match(/#([^-]+-oewn-\d+-n)>/);
      if (match) {
        const synsetId = match[1].replace(/^[^-]+-/, '');
        if (!synsetToLemma.has(synsetId)) {
          synsetToLemma.set(synsetId, currentWrittenRep);
        }
      }
    }
  }

  console.log(`   Found ${synsetToLemma.size} noun lemma mappings`);

  // Second pass: collect ONLY noun synsets (-n) with their data
  const synsets = [];
  let currentSynset = null;
  let inDefinition = false;
  let isNounSynset = false;

  fileStream = createReadStream(filePath);
  rl = createInterface({ input: fileStream, crlfDelay: Infinity });

  for await (const line of rl) {
    const trimmed = line.trim();

    // Start of synset - check if it's a noun
    if (trimmed.startsWith('wnid:oewn-')) {
      // Save previous synset if it was a noun
      if (currentSynset && isNounSynset) {
        synsets.push(currentSynset);
        if (synsets.length >= limit) break;
      }

      // Extract ID and check if it's a noun (-n suffix)
      const match = trimmed.match(/wnid:(oewn-\d+-[a-z])/);
      if (match) {
        const id = match[1];
        isNounSynset = id.endsWith('-n');

        if (isNounSynset) {
          const lemma = synsetToLemma.get(id) || null;
          currentSynset = { id, lemma, definition: null, hypernyms: [] };
          inDefinition = false;
        } else {
          currentSynset = null;
        }
      }
    }

    // Only process if current synset is a noun
    if (!currentSynset || !isNounSynset) continue;

    // Definition start (blank node)
    if (trimmed.includes('wn:definition [')) {
      inDefinition = true;
    }

    // Definition value (inside blank node)
    if (inDefinition && trimmed.includes('rdf:value')) {
      const match = trimmed.match(/"([^"]+)"@en/);
      if (match) {
        currentSynset.definition = match[1];
        inDefinition = false;
      }
    }

    // Hypernym (parent class) - only capture noun hypernyms
    if (trimmed.includes('wn:hypernym')) {
      const match = trimmed.match(/wnid:(oewn-\d+-n)/);
      if (match) {
        currentSynset.hypernyms.push(match[1]);
      }
    }
  }

  // Push last synset if it was a noun
  if (currentSynset && isNounSynset && synsets.length < limit) {
    synsets.push(currentSynset);
  }

  console.log(`   Parsed ${synsets.length} noun synsets`);
  return synsets;
}
